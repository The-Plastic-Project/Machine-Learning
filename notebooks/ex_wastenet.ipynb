{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t268KSlwRmH5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_feature_extractor(input_shape, num_classes):\n",
        "    # Load the DenseNet model pre-trained on ImageNet\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    \n",
        "    # Freeze the convolutional layer blocks and flatten layer\n",
        "    for layer in base_model.layers:\n",
        "        if 'conv' in layer.name or 'pool' in layer.name or 'flatten' in layer.name:\n",
        "            layer.trainable = False\n",
        "    \n",
        "    # Get the output of the last convolutional layer\n",
        "    output = base_model.output\n",
        "    \n",
        "    # Add a global average pooling layer\n",
        "    output = tf.keras.layers.GlobalAveragePooling2D()(output)\n",
        "    \n",
        "    # Add a fully connected layer with softmax activation for classification\n",
        "    output = tf.keras.layers.Dense(num_classes, activation='softmax')(output)\n",
        "    \n",
        "    # Create the feature extraction model\n",
        "    feature_extractor = Model(inputs=base_model.input, outputs=output)\n",
        "    \n",
        "    return feature_extractor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the input shape of your images\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Specify the number of classes in your classification task\n",
        "num_classes = 10\n",
        "\n",
        "# Create the feature extraction model\n",
        "model = create_feature_extractor(input_shape, num_classes)\n",
        "\n",
        "# Compile the model with your choice of optimizer and loss function\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "iiXnc5frRwZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "def fine_tune_model(model, learning_rates):\n",
        "    # Get the total number of layers in the model\n",
        "    num_layers = len(model.layers)\n",
        "    \n",
        "    # Create a list of optimizers with different learning rates for each layer\n",
        "    optimizers = [SGD(learning_rate=lr) for lr in learning_rates]\n",
        "    \n",
        "    # Set the optimizers for each layer\n",
        "    for i, optimizer in enumerate(optimizers):\n",
        "        model.layers[i].trainable = True\n",
        "    \n",
        "    # Compile the model with the final learning rate\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "bXA4qLGKRzK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the learning rates for each layer\n",
        "learning_rates = [1e-4, 1e-5, 1e-6, 1e-7]  # Example learning rates for each layer\n",
        "\n",
        "# Load the pre-trained feature extraction model\n",
        "model = create_feature_extractor(input_shape, num_classes)\n",
        "\n",
        "# Perform fine-tuning on the model\n",
        "model = fine_tune_model(model, learning_rates)"
      ],
      "metadata": {
        "id": "QcP0ZQE7R1CU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "class FeatureExtractorFineTuner:\n",
        "    def __init__(self, input_shape, num_classes, learning_rates):\n",
        "        self.input_shape = input_shape\n",
        "        self.num_classes = num_classes\n",
        "        self.learning_rates = learning_rates\n",
        "        self.feature_extractor = self.create_feature_extractor()\n",
        "        self.fine_tuning_started = False\n",
        "    \n",
        "    def create_feature_extractor(self):\n",
        "        # Load the DenseNet model pre-trained on ImageNet\n",
        "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=self.input_shape)\n",
        "        \n",
        "        # Freeze the convolutional layer blocks and flatten layer\n",
        "        for layer in base_model.layers:\n",
        "            if 'conv' in layer.name or 'pool' in layer.name or 'flatten' in layer.name:\n",
        "                layer.trainable = False\n",
        "        \n",
        "        # Get the output of the last convolutional layer\n",
        "        output = base_model.output\n",
        "        \n",
        "        # Add a global average pooling layer\n",
        "        output = tf.keras.layers.GlobalAveragePooling2D()(output)\n",
        "        \n",
        "        # Add a fully connected layer with softmax activation for classification\n",
        "        output = tf.keras.layers.Dense(self.num_classes, activation='softmax')(output)\n",
        "        \n",
        "        # Create the feature extraction model\n",
        "        feature_extractor = Model(inputs=base_model.input, outputs=output)\n",
        "        \n",
        "        return feature_extractor\n",
        "    \n",
        "    def fine_tune_model(self):\n",
        "        # Get the total number of layers in the model\n",
        "        num_layers = len(self.feature_extractor.layers)\n",
        "        \n",
        "        # Create a list of optimizers with different learning rates for each layer\n",
        "        optimizers = [SGD(learning_rate=lr) for lr in self.learning_rates]\n",
        "        \n",
        "        # Set the optimizers for each layer\n",
        "        for i, optimizer in enumerate(optimizers):\n",
        "            self.feature_extractor.layers[i].trainable = True\n",
        "        \n",
        "        # Compile the model with the final learning rate\n",
        "        self.feature_extractor.compile(optimizer=optimizers[-1], loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    def train(self, train_data, train_labels, epochs=10):\n",
        "        if not self.fine_tuning_started:\n",
        "            # Perform feature extraction\n",
        "            optimizer = SGD(learning_rate=self.learning_rates[0])\n",
        "            self.feature_extractor.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            self.feature_extractor.fit(train_data, train_labels, epochs=epochs)\n",
        "            \n",
        "            # Check the accuracy threshold to start fine-tuning\n",
        "            accuracy = self.feature_extractor.history.history['accuracy'][-1]\n",
        "            if accuracy >= 0.6 and accuracy <= 0.7:\n",
        "                self.fine_tune_model()\n",
        "                self.fine_tuning_started = True\n",
        "        else:\n",
        "            # Fine-tuning\n",
        "            optimizer = SGD(learning_rate=self.learning_rates[-1])\n",
        "            self.feature_extractor.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            self.feature_extractor.fit(train_data, train_labels, epochs=epochs)\n"
      ],
      "metadata": {
        "id": "Cu-ziB9uR3NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert class labels to one-hot encoded vectors\n",
        "num_classes = 10\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# Define the input shape for the feature extractor\n",
        "input_shape = x_train.shape[1:]\n",
        "\n",
        "# Define the learning rates for fine-tuning\n",
        "learning_rates = [0.001, 0.001, 0.0001]\n",
        "\n",
        "# Create an instance of FeatureExtractorFineTuner\n",
        "extractor_fine_tuner = FeatureExtractorFineTuner(input_shape, num_classes, learning_rates)\n",
        "\n",
        "# Train the model\n",
        "extractor_fine_tuner.train(x_train, y_train, epochs=20)\n"
      ],
      "metadata": {
        "id": "rh9CkVDaR5mu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}